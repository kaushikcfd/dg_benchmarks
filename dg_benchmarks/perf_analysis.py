from functools import cache
from meshmode.array_context import (
    # TODO rename FusionContractorArrayContext to
    # BatchedEinsumPytatoPyOpenCLArrayContext when mirgecom production
    # is using up to date meshmode.
    FusionContractorArrayContext as BatchedEinsumArrayContext)
from typing import Optional

import numpy as np
import loopy as lp
import pyopencl as cl
import pytato


# {{{ actx to get the kernel with loop fusion, contraction

class _MinimalBytesKernelException(RuntimeError):
    pass


class _MinimalFlopsKernelException(RuntimeError):
    pass


class _BatchedEinsumKernelWithMinimalBytesGettingActx(BatchedEinsumArrayContext):
    """
    An :class:`~arraycontext.ArrayContext` which on obtaining
    :class:`lp.TranslationUnit` in :meth:`transform_loopy_program` which
    has been generated by :meth:`~arraycontext.ArrayContext.compile` raises a
    :class:`_MinimalBytesKernelException` with the translation unit as the
    argument.
    """
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    def transform_loopy_program(self, t_unit):
        from arraycontext.impl.pytato.compile import FromArrayContextCompile
        if t_unit.default_entrypoint.tags_of_type(FromArrayContextCompile):

            # loop fusion
            from meshmode.arraycontext_extras.batched_einsum.utils import (
                apply_kennedy_fusion_with_batched_einsum_extension)
            t_unit = apply_kennedy_fusion_with_batched_einsum_extension(
                t_unit, self.loop_fusion_axis_tag_t,
                self.fused_loop_name_prefix_getter)

            # array contraction
            from meshmode.arraycontext_extras.batched_einsum.utils import (
                contract_arrays)
            t_unit = contract_arrays(t_unit)

            raise _MinimalBytesKernelException(t_unit)
        else:
            return super().transform_loopy_program(t_unit)


class _BatchedEinsumKernelWithMinimalFlopsGettingActx(BatchedEinsumArrayContext):
    """
    An :class:`~arraycontext.ArrayContext` which on obtaining
    :class:`lp.TranslationUnit` in :meth:`transform_loopy_program` which
    has been generated by :meth:`~arraycontext.ArrayContext.compile` raises a
    :class:`_MinimalBytesKernelException` with the translation unit as the
    argument.
    """
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    def transform_dag(self,
                      dag: "pytato.DictOfNamedArrays") -> "pytato.DictOfNamedArrays":
        import pytato as pt

        from meshmode.arraycontext_extras.batched_einsum.utils import (
            _make_passthrough_arg,
            get_indirection_maps,
            get_inputs_and_outputs_of_reduction_nodes)
        from meshmode.arraycontext_extras.split_actx.utils import (
            get_inputs_and_outputs_of_einsum)

        # Step 1. Collapse equivalent nodes in DAG.
        # -----------------------------------------
        # type-ignore-reason: mypy is right pytato provides imprecise types.
        dag = pt.transform.deduplicate_data_wrappers(dag)  # type: ignore[assignment]

        # Step 2. Materialize einsum/reduction outputs.
        # ---------------------------------------------
        einsum_inputs, einsum_outputs = get_inputs_and_outputs_of_einsum(dag)
        reduction_inputs, reduction_outputs = (
            get_inputs_and_outputs_of_reduction_nodes(dag))

        def materialize_all_einsums_or_reduces(expr):
            if (expr in einsum_outputs
                    or expr in reduction_outputs
                    or expr in einsum_inputs
                    or expr in reduction_inputs):
                return expr.tagged(pt.tags.ImplStored())
            else:
                return expr

        # type-ignore-reason: mypy is right pytato provides imprecise types.
        dag = pt.transform.map_and_copy(dag,  # type: ignore[assignment]
                                        materialize_all_einsums_or_reduces)

        # Step 3. Materialize with MPMS
        # -----------------------------
        dag = pt.transform.materialize_with_mpms(dag)

        # Step 4. Mark all indirection maps as non-negative
        # -------------------------------------------------
        if self.assume_all_indirection_maps_as_non_negative:
            indirection_maps = get_indirection_maps(dag)

            def tag_indices_as_non_negative(ary):
                if ary in indirection_maps:
                    return ary.tagged(pt.tags.AssumeNonNegative())
                else:
                    return ary

            # type-ignore-reason: mypy is right pytato provides imprecise types.
            dag = pt.transform.map_and_copy(dag,  # type: ignore[assignment]
                                            tag_indices_as_non_negative)

        # Step 5. Get rid of broadcasts in einsum expressions (helps feinsum)
        # -------------------------------------------------------------------
        dag = pt.rewrite_einsums_with_no_broadcasts(dag)

        # Step 6. Infer axis tags
        # -----------------------
        # type-ignore-reason: mypy is right pytato provides imprecise types.
        dag = pt.unify_axes_tags(dag)  # type: ignore[assignment]

        # Step 7. Make all pt.einsum/pt.reduction inputs as substitutions
        # ---------------------------------------------------------------
        def implement_einsum_reduction_inputs_as_substs(expr):
            from immutables import Map

            from pytato.target.loopy import ImplSubstitution
            if isinstance(expr, pt.Einsum):
                # make the arguments passthrough to make use of already stored
                # values.
                # pylint and 'attrs' have poor compatibility
                # pylint: disable=too-many-function-args,redundant-keyword-arg
                # pylint: disable=unexpected-keyword-arg
                return pt.Einsum(
                    expr.access_descriptors,
                    tuple(_make_passthrough_arg(arg, ImplSubstitution())
                          for arg in expr.args),
                    expr.redn_axis_to_redn_descr,
                    expr.index_to_access_descr,
                    tags=expr.tags,
                    axes=expr.axes,
                )
            elif isinstance(expr, pt.IndexLambda) and expr.var_to_reduction_descr:
                # make the arguments passthrough to make use of already stored
                # values.
                # pylint: disable=too-many-function-args,redundant-keyword-arg
                # pylint: disable=unexpected-keyword-arg
                return pt.IndexLambda(
                    expr.expr,
                    expr.shape,
                    expr.dtype,
                    Map({name: _make_passthrough_arg(bnd, ImplSubstitution())
                         for name, bnd in expr.bindings.items()}),
                    expr.var_to_reduction_descr,
                    tags=expr.tags,
                    axes=expr.axes,
                )
            else:
                return expr

        # type-ignore-reason: mypy is right pytato provides imprecise types.
        dag = pt.transform.map_and_copy(dag,  # type: ignore[assignment]
                                        implement_einsum_reduction_inputs_as_substs)

        return dag

    def transform_loopy_program(self, t_unit):
        from arraycontext.impl.pytato.compile import FromArrayContextCompile
        if t_unit.default_entrypoint.tags_of_type(FromArrayContextCompile):

            # loop fusion
            from meshmode.arraycontext_extras.batched_einsum.utils import (
                apply_kennedy_fusion_with_batched_einsum_extension)
            t_unit = apply_kennedy_fusion_with_batched_einsum_extension(
                t_unit, self.loop_fusion_axis_tag_t,
                self.fused_loop_name_prefix_getter)

            # array contraction
            from meshmode.arraycontext_extras.batched_einsum.utils import (
                contract_arrays)
            t_unit = contract_arrays(t_unit)

            raise _MinimalFlopsKernelException(t_unit)
        else:
            return super().transform_loopy_program(t_unit)

# }}}


@cache
def _get_min_flops_kernel(equation: str,
                          dim: int,
                          degree: int) -> lp.TranslationUnit:
    from meshmode.dof_array import array_context_for_pickling
    from dg_benchmarks.utils import (get_benchmark_rhs_invoker,
                                     get_benchmark_ref_input_arguments_path,
                                     is_dataclass_array_container)
    rhs_invoker = get_benchmark_rhs_invoker(equation, dim, degree)
    cl_ctx = cl.create_some_context()
    cq = cl.CommandQueue(cl_ctx)

    actx = _BatchedEinsumKernelWithMinimalFlopsGettingActx(cq)
    rhs_clbl = rhs_invoker(actx)

    with open(get_benchmark_ref_input_arguments_path(equation, dim, degree),
              "rb") as fp:
        import pickle
        with array_context_for_pickling(actx):
            np_args, np_kwargs = pickle.load(fp)

    if (all((is_dataclass_array_container(arg)
             or (isinstance(arg, np.ndarray)
                 and arg.dtype == "O"
                 and all(is_dataclass_array_container(el)
                         for el in arg))
             or np.isscalar(arg))
            for arg in np_args)
            and all(is_dataclass_array_container(arg) or np.isscalar(arg)
                    for arg in np_kwargs.values())):
        args, kwargs = np_args, np_kwargs
    elif (any(is_dataclass_array_container(arg) for arg in np_args)
            or any(is_dataclass_array_container(arg)
                   for arg in np_kwargs.values())):
        raise NotImplementedError("Pickling not implemented for input"
                                  " types.")
    else:
        args, kwargs = (tuple(actx.from_numpy(arg) for arg in np_args),
                        {kw: actx.from_numpy(arg) for kw, arg in np_kwargs.items()})

    try:
        rhs_clbl(*args, **kwargs)
    except _MinimalFlopsKernelException as e:
        t_unit, = e.args
        assert isinstance(t_unit, lp.TranslationUnit)
        return t_unit
    else:
        raise RuntimeError("Was expecting a 'MinimalBytesKernelException'")


@cache
def _get_min_bytes_kernel(equation: str,
                          dim: int,
                          degree: int) -> lp.TranslationUnit:
    from meshmode.dof_array import array_context_for_pickling
    from dg_benchmarks.utils import (get_benchmark_rhs_invoker,
                                     get_benchmark_ref_input_arguments_path,
                                     is_dataclass_array_container)
    rhs_invoker = get_benchmark_rhs_invoker(equation, dim, degree)
    cl_ctx = cl.create_some_context()
    cq = cl.CommandQueue(cl_ctx)

    actx = _BatchedEinsumKernelWithMinimalBytesGettingActx(cq)
    rhs_clbl = rhs_invoker(actx)

    with open(get_benchmark_ref_input_arguments_path(equation, dim, degree),
              "rb") as fp:
        import pickle
        with array_context_for_pickling(actx):
            np_args, np_kwargs = pickle.load(fp)

    if (all((is_dataclass_array_container(arg)
             or (isinstance(arg, np.ndarray)
                 and arg.dtype == "O"
                 and all(is_dataclass_array_container(el)
                         for el in arg))
             or np.isscalar(arg))
            for arg in np_args)
            and all(is_dataclass_array_container(arg) or np.isscalar(arg)
                    for arg in np_kwargs.values())):
        args, kwargs = np_args, np_kwargs
    elif (any(is_dataclass_array_container(arg) for arg in np_args)
            or any(is_dataclass_array_container(arg)
                   for arg in np_kwargs.values())):
        raise NotImplementedError("Pickling not implemented for input"
                                  " types.")
    else:
        args, kwargs = (tuple(actx.from_numpy(arg) for arg in np_args),
                        {kw: actx.from_numpy(arg) for kw, arg in np_kwargs.items()})

    try:
        rhs_clbl(*args, **kwargs)
    except _MinimalBytesKernelException as e:
        t_unit, = e.args
        assert isinstance(t_unit, lp.TranslationUnit)
        return t_unit
    else:
        raise RuntimeError("Was expecting a 'MinimalBytesKernelException'")


@cache
def get_float64_flops(equation: str, dim: int, degree: int) -> int:
    import feinsum as fnsm
    from meshmode.arraycontext_extras.split_actx.utils import (
        InsnIds, _get_call_kernel_insn_ids)

    t_unit = _get_min_flops_kernel(equation, dim, degree)
    t_unit = t_unit.with_kernel(
        t_unit.default_entrypoint.copy(
            silenced_warnings=["insn_count_subgroups_upper_bound",
                               "summing_if_branches_ops",
                               ],
        )
    )

    elementwise_insn_ids = frozenset({
        insn.id
        for insn in t_unit.default_entrypoint.instructions
        if not insn.reduction_inames()})
    op_map = lp.get_op_map(t_unit,
                           subgroup_size=1,
                           within=InsnIds(elementwise_insn_ids))

    kernel_name = t_unit.default_entrypoint.name

    # {{{ compute FLOPs from elementwise insns

    c128_ops = {op_type: (op_map.filter_by(dtype=[np.complex128],
                                           name=op_type,
                                           kernel_name=kernel_name)
                          .eval_and_sum({}))
                for op_type in ["add", "mul", "div"]}
    f64_ops = (op_map.filter_by(dtype=[np.float64],
                                kernel_name=kernel_name).eval_and_sum({})
               + (2 * c128_ops["add"]
                  + 6 * c128_ops["mul"]
                  + (6 + 3 + 2) * c128_ops["div"]))

    c64_ops = {op_type: (op_map.filter_by(dtype=[np.complex64],
                                          name=op_type,
                                          kernel_name=kernel_name)
                         .eval_and_sum({}))
               for op_type in ["add", "mul", "div"]}
    f32_ops = (op_map.filter_by(dtype=[np.float32],
                                kernel_name=kernel_name).eval_and_sum({})
               + (2 * c64_ops["add"]
                  + 6 * c64_ops["mul"]
                  + (6 + 3 + 2) * c64_ops["div"]))

    # }}}

    call_kernel_insn_ids = _get_call_kernel_insn_ids(t_unit[kernel_name])

    for insn_ids in call_kernel_insn_ids:
        within_inames, = {t_unit[kernel_name].id_to_insn[insn_id].within_inames
                          for insn_id in insn_ids}
        redn_inames, = {t_unit[kernel_name].id_to_insn[insn_id].reduction_inames()
                        for insn_id in insn_ids}
        if redn_inames:
            from feinsum.measure import _get_giga_ops_from_einsum
            einsum, _ = fnsm.get_a_matched_einsum(t_unit,
                                                  insn_match=InsnIds(insn_ids),
                                                  kernel_name=kernel_name,
                                                  long_dim_length=np.inf)
            for dtype, gops in _get_giga_ops_from_einsum(einsum).items():
                if dtype == np.float32:
                    f32_ops += gops * 1e9
                elif dtype == np.float64:
                    f64_ops += gops * 1e9
                else:
                    raise NotImplementedError(dtype)

    if f32_ops:
        print(f32_ops)
        raise RuntimeError("Single precision FLOPS != 0 => failed assumption.")

    return f64_ops


@cache
def get_footprint_bytes(equation: str, dim: int, degree: int) -> int:
    from pytools import product
    from loopy.kernel.array import ArrayBase

    t_unit = _get_min_bytes_kernel(equation, dim, degree)

    knl = t_unit.default_entrypoint
    nfootprint_bytes = 0

    for ary in knl.args:
        if (isinstance(ary, ArrayBase)
                and ary.address_space == lp.AddressSpace.GLOBAL):
            nfootprint_bytes += (product(ary.shape)
                                 * ary.dtype.itemsize)

    for ary in knl.temporary_variables.values():
        if ary.address_space == lp.AddressSpace.GLOBAL:
            # global temps would be written once and read once
            nfootprint_bytes += (2 * product(ary.shape)
                                 * ary.dtype.itemsize)

    return nfootprint_bytes


@cache
def get_roofline_flop_rate(equation: str, dim: int, degree: int,
                           roofline_model: str = "batched_einsum:global_ai",
                           device_name: Optional[str] = None,
                           ) -> float:
    from dg_benchmarks.consts import (DEV_TO_PEAK_BW,
                                      DEV_TO_PEAK_F64_GFLOPS)

    if roofline_model == "batched_einsum:global_ai":
        import pyopencl as cl
        cl_ctx = cl.create_some_context()
        if device_name is None:
            device_name, = {dev.name for dev in cl_ctx.devices}

        nflops = get_float64_flops(equation, dim, degree)
        nbytes = get_footprint_bytes(equation, dim, degree)

        try:
            t_runtime = max(
                ((nflops*1e-9)
                 / DEV_TO_PEAK_F64_GFLOPS[device_name]),
                ((nbytes*1e-9)
                 / DEV_TO_PEAK_BW[device_name])
            )
        except KeyError:
            return np.nan
        else:
            return get_float64_flops(equation, dim, degree)/t_runtime
    else:
        raise NotImplementedError("Unknown roofline model:", roofline_model)
